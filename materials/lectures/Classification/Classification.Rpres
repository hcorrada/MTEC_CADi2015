Classification
========================================================
author: Hector Corrada Bravo
date: CMSC498T: Intro Data Science

Some Classification Problems
========================================================

```{r, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

Predict the type of computer system a programmer prefers. The response would be Windows, Mac, or Linux. The predictors could be favorite programming languages, annual salary, whether the programmer comments his code, and his geographic proximity to Redmond, Washington. This is a prediction problem.


<small>[from your submissions]</small>

Some Classification Problems
========================================================

- Response: Success of a tech start up
- Predictors: Market it's trying to get into. Funding it can raise. Personality of Founder. Participation in incubators. I have no idea.
- Finite possibilities, so classification. People want to understand what makes a buisness successful, so inference.

<small>[from your submissions]</small>

Some Classification Problems
========================================================

Mode choice of an individual to commute to work. Predictors: income, cost and time required for each of the alternatives: driving/carpooling,  biking, taking a bus, taking the train. Response: whether the individual makes their commute by car, bike, bus or train. Inference - shows how people value the price and time when considering their mode choice. 

<small>[from your submissions]</small>

Classification
===============

Can we predict $Y$, taking values from a set of classes, from predictors $\mathbf{X}$?

$(\mathbf{x}_1, y_1), \ldots, (\mathbf{x}_n, y_n)$ Training data as in regression

Goal is to predict **accurately** on **unseen** data.

Classification
===============

![](4_1a.png)

***

![](4_1b.png)

Why not linear regression?
============================

We discussed this in "Intro to Statistical Learning" lecture

For multiple possible classes, if order and scale (units) don't make sense, then it's not a regression problem

$$
Y = 
\begin{cases}
1 & \textrm{if } \mathtt{stroke} \\
2 & \textrm{if } \mathtt{drug overdose} \\
3 & \textrm{if } \mathtt{epileptic seizure}
\end{cases}
$$

Why not linear regression?
===========================

For **binary** responses, it's a little better:

$$
Y = 
\begin{cases}
0 & \textrm{if } \mathtt{stroke} \\
1 & \textrm{if } \mathtt{drug overdose} \\
\end{cases}
$$

Fit with linear regression and _interpret_ as probability (e.g, if $\hat{y} > 0.5$ predict $\mathtt{drug overdose}$)

Why not linear regression?
=============================

![](4_2.png)

Classification as probability estimation problem
=================================================

- Instead of modeling classes 0 or 1 directly, let's model $P(Y=1|X)$, and classify based on this probability.

- In general, classification approaches use _discriminant_ (think of _scoring_) functions to do classification.

- Logistic regression is **one** way of estimating this class probability $P(Y=1|X)$ (also denoted $p(x)$)

Classification as probability estimation problem
==================================================

```{r,echo=FALSE}
library(MASS)

library(RColorBrewer)
mycols <- brewer.pal(8, "Dark2")[c(3,2)]

s <- sqrt(1/5)
set.seed(30)

makeX <- function(M, n=100, sigma=diag(2)*s) {
  z <- sample(1:nrow(M), n, replace=TRUE)
  m <- M[z,]
  return(t(apply(m,1,function(mu) mvrnorm(1,mu,sigma))))
}

M0 <- mvrnorm(10, c(1,0), diag(2)) # generate 10 means
x0 <- makeX(M0) ## the final values for y0=blue

M1 <- mvrnorm(10, c(0,1), diag(2))
x1 <- makeX(M1)

x <- rbind(x0, x1)
y <- c(rep(0,100), rep(1,100))
cols <- mycols[y+1]

GS <- 75 # put data in a Gs x Gs grid
XLIM <- range(x[,1])
tmpx <- seq(XLIM[1], XLIM[2], len=GS)

YLIM <- range(x[,2])
tmpy <- seq(YLIM[1], YLIM[2], len=GS)

newx <- expand.grid(tmpx, tmpy)
colnames(newx) <- c("X1","X2")
```

```{r, echo=FALSE, fig.height=10, fig.width=10}
layout(matrix(1:4, nr=2, byrow=FALSE))
plot(x, col=cols, xlab="X1", ylab="X2", xlim=XLIM, ylim=YLIM, type="n", main="Training Set")
points(x, col=cols)

# logistic regression
dat <- data.frame(X1=x[,1], X2=x[,2])
fit <- glm(y~X1+X2, data=dat,family=binomial)
yhat <- predict(fit, newdata=newx)
yhat <- ifelse(yhat > 0, 2, 1)
colshat <- mycols[yhat]

coefs <- coef(fit)
a <- -coefs[1] / coefs[3]
b <- -coefs[2] / coefs[3]

plot(x, col=cols, xlab="X1", ylab="X2", xlim=XLIM, ylim=YLIM, type="n",main="logistic regression")
points(x, col=cols)
points(newx, col=colshat, pch=".")
abline(a=a,b=b)

# KNN(15)
library(class)
yhat <- knn(x, newx, y, k=15)
colshat <- mycols[as.numeric(yhat)]
plot(x, col=cols, xlab="X1", ylab="X2", xlim=XLIM, ylim=YLIM, type="n",main="KNN(15)")
points(x, col=cols)
points(newx, col=colshat, pch=".")
contour(tmpx, tmpy, matrix(as.numeric(yhat),GS,GS), levels=c(1,2), add=TRUE, drawlabels=FALSE)

# KNN(1)
yhat <- knn(x, newx, y, k=1)
colshat <- mycols[as.numeric(yhat)]
plot(x, col=cols, xlab="X1", ylab="X2", xlim=XLIM, ylim=YLIM, type="n",main="KNN(1)")
points(x, col=cols)
points(newx, col=colshat, pch=".")
contour(tmpx, tmpy, matrix(as.numeric(yhat),GS,GS), levels=c(1,2), add=TRUE, drawlabels=FALSE)
```

Logistic regression
====================

- Basic idea is to build a **linear** model _related_ to $p(x)$, but linear regression directly (i.e. $p(x) = \beta_0 + \beta_1 x$) doesn't work. Why?

- Instead use _log-odds_:

$$
\log \frac{p(x)}{1-p(x)} = \beta_0 + \beta_1 x
$$

- Odds: ratio of probabilities
  - "two to one odds that Ted Cruz wins presidency" means "the probability that Ted Cruz wins is double the probability he loses"
  - So if odds = 2, $p(x)=2/3$. If odds = 1/2, $p(x)=1/3$. In general odds = $\frac{p(x)}{1-p(x)}$.
  
Logistic regression
=====================

From quiz:

1. Suppose an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?

2. On average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?

Logistic regression
=====================

```{r, echo=FALSE, results="hide", fig.width=10, fig.height=10}
library(ISLR)
data(Default)

fit <- glm(default~balance, data=Default, family=binomial)
ilogis <- function(theta) exp(theta) / (1 + exp(theta))

makeplot <- function(beta1) {
  with(Default,
       plot(balance, as.numeric(default)-1,
        ylab="Probability of default",
        main=substitute(list(hat(beta)[0]==beta0, hat(beta)[1]==beta1), 
                        list(beta0=round(coef(fit)[1], digits=2), beta1=round(beta1, digits=3)))))
  curve(ilogis(coef(fit)[1] + beta1 * x), add=TRUE, col="blue", lwd=1.3)
  abline(h=c(0,1), lty=2)
}

layout(matrix(1:4, nr=2, byrow=TRUE))
sapply(c(0.001, coef(fit)[2], 0.01, 0.1), makeplot)
```


Logistic regression
=====================

```{r, results="asis"}
fit <- glm(default ~ balance, data=Default, family=binomial)
kable(summary(fit)$coef, digits=4)
```

Interpretation:
 - the **odds** that person defaults increase by $e^{0.05}$ for every dollar in balance
 - The **accuracy** of $\hat{\beta}_1$ as an estimate of the **population** parameter is given Std. Error

Logistic regression
=====================

```{r, results="asis", echo=FALSE}
fit <- glm(default ~ balance, data=Default, family=binomial)
kable(summary(fit)$coef, digits=4)
```

Interpretation:
 - Z-value $\frac{\hat{\beta}_1}{\mathrm{SE}(\hat{\beta}_1)}$ plays the role of the t-statistic in linear regression: a scaled measure of our estimate (signal / noise)
 - The P-value is the probability of seeing a Z-value as large (e.g., 24.95) under the null hypothesis that **there is no relationship between balance and the probability of defaulting**, i.e., $\beta_1=0$ in the population
 
Logistic regression
=====================

- Again, an algorithm required to _estimate_ parameters $\beta_0$ and $\beta_1$.
- In logistic regression we use a **binomial** probability model: think of flipping a coin weighted by $p(x)$
- We _estimate_ parameters to **maximize** the likelihood of the observed training data under this coin flipping (binomial) model
- I.e.: solve the following optimization problem

$$
\max_{\beta_0, \beta_1} \sum_{i:\, y_i=1} log(p(x_i)) + \sum_{i: y_i=0} log(1-p(x_i))
$$

- Nonlinear (but convex problem), you can learn algorithms to solve it in "Computational Methods" class (CMSC 460)

Logistic regression
=====================

```{r, results="asis", echo=FALSE}
fit <- glm(default ~ balance, data=Default, family=binomial)
kable(summary(fit)$coef, digits=4)
```

Making predictions: 

On average, the probability that a person with a balance of $1,000 defaults is:
 
$$
\hat{p}(1000) = \frac{e^{\hat{\beta}_0 + \hat{\beta}_1 \times 1000}}{1+e^{\beta_0 + \beta_1 \times 1000}} 
\approx \frac{e^{-10.6514 + 0.0055 \times 1000}}{1+e^{-10.6514 + 0.0055 \times 1000}} \\
\approx 0.00576 
$$
 
 
Multiple logistic regression
===============================
 
Classification analog to linear regression:

$$
\log \frac{p(\mathbf{x})}{1-p(\mathbf{x})} = \beta_0 + \beta_1 x_1 + \cdots + \beta_p x_p
$$

```{r, results="asis"}
fit <- glm(default ~ balance + income + student, data=Default, family="binomial")
kable(summary(fit)$coef, digits=4)
```

Multiple logisitic regression
==============================

Essential to avoid **confounding!**

Single logistic regression of default vs. student status:

```{r, echo=FALSE, results="asis"}
fit1 <- glm(default ~ student, data=Default, family="binomial")
kable(summary(fit1)$coef, digits=4)
```

Multiple logistic regression:
```{r, echo=FALSE, results="asis"}
fit2 <- glm(default ~ balance + income + student, data=Default, family="binomial")
kable(summary(fit2)$coef, digits=4)
```

Multiple logistic regression
==============================

```{r, echo=FALSE}
bal_range <- range(Default$balance)
plot(0,0,xlim=bal_range,ylim=c(0,1),xlab="Credit Card Balance", ylab="Default Rate", type="n")
curve(predict(fit1, newdata=data.frame(student="Yes", balance=x), type="response"), add=TRUE, lty=2, col="orange", lwd=1.6)
curve(predict(fit1, newdata=data.frame(student="No", balance=x), type="response"), add=TRUE, lty=2, col="blue", lwd=1.6)
curve(predict(fit2, newdata=data.frame(student="Yes", balance=x, income=mean(Default$income)), type="response"), add=TRUE, lty=1, col="orange", lwd=1.6)
curve(predict(fit2, newdata=data.frame(student="No", balance=x, income=mean(Default$income)), type="response"), add=TRUE, lty=1, col="blue", lwd=1.6)
```

***

```{r, echo=FALSE}
boxplot(balance~student, data=Default, col=c("blue", "orange"), xlab="Student Status", ylab="Credit Card Balance")
```

Multiple Logistic Regression
==============================

From quiz:

1. Suppose we collect data for a group of students in a statistics class with variables X1 = hours studied, X2 = undergrad GPA, and Y = receive an A. We fit a logistic regression and produce estimated coefficients, $\hat{\beta}_0=-6, \hat{\beta}_1=0.05,\hat{\beta}_2=1$.

  Estimate the probability that a student who studies for 40h and has an undergraduate GPA of 3.5 gets an A in the class.

2. With estimated parameters from previous question, and GPA of 3.5 as before, how many hours would the student need to study to have a 50% chance of getting an A in the class?

Linear Discriminant Analysis
=========================================

- Another linear method, based on probability model.
- Recall that we want to partition data based on class probability: _find the $\mathbf{X}$ for which $P(\mathrm{default=Yes}|X) > P(\mathrm{default=No}|X)$ 
- In logistic regression **we made no assumption about $\mathbf{X}$**
- In some cases, we **can** make assumptions about $\mathbf{X}$ that help classification (if assumptions hold, obviously)

Linear Discriminant Analysis
==============================

```{r, echo=FALSE, fig.width=10, fig.height=10}
layout(matrix(1:4,nr=2))
hist(Default$balance[Default$default=="Yes"], main="Balance distribution if defaulting")
qqnorm(Default$balance[Default$default=="Yes"])


hist(Default$balance[Default$default=="No"], main = "Balance distribution if not defaulting")
qqnorm(Default$balance[Default$default=="No"])
```

Next time
==========

- Fitting LDA
- Assessing classification accuracy
- Comparing models via resampling (ISL Ch. 5)
