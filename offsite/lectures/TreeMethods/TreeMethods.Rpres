TreeMethods
========================================================
author: Hector Corrada Bravo
date: CMSC498T Intro Data Science

```{r, echo=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

Tree-Based Methods
====================

Very popular, well-known and studied methods in Statistical Learning

- Decision Tree Models
- Random Forests

Regression Trees
==================

```{r, echo=FALSE}
library(tree)
library(ISLR)
library(RColorBrewer)
palette(brewer.pal(8, "Dark2"))
data(Auto)


with(Auto, plot(horsepower, mpg, pch=19, cex=1.4))
legend("topright", pch=19, col=1:3, legend=1:3)
```

***

```{r, echo=FALSE}
tree <- tree(mpg~horsepower, data=Auto)
plot(tree)
text(tree, pretty=0, cex=1.3)
```

Regression Trees
==================

```{r, echo=FALSE, cache=FALSE, results="hide"}
library(RColorBrewer)
palette(brewer.pal(8, "Dark2"))

with(Auto, plot(horsepower, mpg, pch=19, cex=1.4))
#abline(h=subset(tree$frame, grepl("leaf", tree$frame$var))$yval)
abline(v=as.numeric(gsub("<", "", subset(tree$frame, !grepl("leaf", tree$frame$var))$splits[,"cutleft"])))

process_node <- function(i, left, right) {
 if (tree$frame$var[i] == "<leaf>") {
   val <- as.numeric(tree$frame$yval[i])
   segments(left, val, right, val, col="red", lwd=5)
 } else {
   val <- as.numeric(gsub("<","",tree$frame$splits[i, "cutleft"]))
   i <- process_node(i+1, left, val)
   i <- process_node(i+1, val, right)
 }
 i
}

process_node(1, .85*min(Auto$horsepower), 1.05*max(Auto$horsepower))
```

***

```{r, echo=FALSE}
tree <- tree(mpg~horsepower, data=Auto)
plot(tree)
text(tree, pretty=0, cex=1.3)
```

Prediction by space partitioning
=================================

Prediction by partitioning feature (predictor) space.

1. Partition space into $J$ non-overlapping regions, $R_1, R_2, \ldots, R_J$.
2. For every observation that falls within region $R_j$, predict response as mean of response for training observations in $R_j$.

**Regression Trees create partition recursively**

Regression Trees
===================

1. Find predictor $j$ and value $s$ that minimize RSS:

$$
\sum_{i:\, x_i \in R_1(j,s))} (y_i - \hat{y}_{R_1})^2 +
\sum_{i:\, x_i \in R_2(j,s))} (y_i - \hat{y}_{R_2})^2
$$

Where $R_1$ and $R_2$ are regions resulting from splitting observations on predictor $j$ and value $s$:

$$
R_1(j,s) = \{X|X_j < s\} \mathrm{ and } R_2(j,s) \{X|X_j \geq s\}
$$

$\hat{y}_{R_j}$ is the mean of the response $Y$ of observations in $R_j$.

Regression Trees
=================

![](8.3.png)

Regression Trees
=================

```{r, echo=FALSE}
with(Auto, {
     plot(horsepower, weight, cex=mpg/median(mpg), pch=19)

    qs <- quantile(mpg, p=seq(0,1, len=5))
    legend("bottomright", pch=19, legend=qs, pt.cex=qs/median(mpg))
})
```

***

```{r, echo=FALSE}
tree <- tree(mpg~horsepower+weight, data=Auto)
plot(tree)
text(tree, pretty=0)
```

Regression Trees
=================

```{r, echo=FALSE, cache=FALSE}
process_node <- function(i, j, left, right, bottom, top, dat) {
  var <- as.character(tree$frame$var[i])
  is_leaf <- grepl("leaf", var)
  
  if (is_leaf) {
    val <- as.numeric(tree$frame$yval[i])
    dat[j,] <- c(j, left, right, bottom, top, val)
    j <- j + 1
  } else {
    val <- as.numeric(gsub("<","",tree$frame$splits[i, "cutleft"]))
    if (var == "horsepower") {
      res <- process_node(i+1, j, left, val, bottom, top, dat)
      i <- res$i; j <- res$j; dat <- res$dat
      res <- process_node(i+1, j, val, right, bottom, top, dat)
      i <- res$i; j <- res$j; dat <- res$dat
    } else {
      res <- process_node(i+1, j, left, right, bottom, val, dat)
      i <- res$i; j <- res$j; dat <- res$dat
      res <- process_node(i+1, j, left, right, val, top, dat)
      i <- res$i; j <- res$j; dat <- res$dat
    }
  }
  list(i=i, j=j, dat=dat)
}

nleaves <- sum(grepl("leaf", tree$frame$var))
region_dat <- data.frame(j=integer(nleaves),
                  left=numeric(nleaves),
                  right=numeric(nleaves),
                  bottom=numeric(nleaves),
                  top=numeric(nleaves),
                  val=numeric(nleaves))

res <- process_node(1, 1, .85*min(Auto$horsepower), 1.05*max(Auto$horsepower), .85*min(Auto$weight), 1.05*max(Auto$weight), region_dat)
region_dat <- res$dat

with(Auto, {
     plot(horsepower, weight, cex=mpg/median(mpg), pch=19)

    qs <- quantile(mpg, p=seq(0,1, len=5))
    legend("bottomright", pch=19, legend=qs, pt.cex=qs/median(mpg))
})

with(region_dat, {
  segments(left, bottom, right, bottom)
  segments(left, top, right, top)
  segments(left, bottom, left, top)
  segments(right, bottom, right, top)
  text(.5*(left+right), .5*(top+bottom), labels=j, cex=4, col="red")
})
```

***

```{r, echo=FALSE}
plot(tree)
text(tree, pretty=0)
```

Regression Trees
=================

In R, built with similar API as linear models

```{r, eval=FALSE}
library(tree)
library(ISLR)
data(Auto)

tree_fit <- tree(mpg~horsepower+weight, data=Auto)
predict(tree_fit)
```

Regression Trees
=================

- When do we stop partitioning? When adding a partition does not reduce RSS, or, when partition has too few training observations.
- Even then, trees built with this stopping criterion tend to _overfit_ training data.
- To avoid this, a post-processing step called _pruning_ is used to make the tree smaller:
  - **Question:** why would a smaller tree tend to generalize better?

Pruning Regression Trees
=========================

Given a large tree $T_0$, _cost complexity pruning_ looks for the subtree $T_{\alpha}$ that minimizes

$$
\sum_{m=1}^{|T|} \sum_{i:\, x_i \in R_m} (y_i-\hat{y}_{R_m})^2 + \alpha |T|
$$

given some positive value $\alpha$.

How to choose $\alpha$? Use cross-validation to select $\alpha$ that minimizes test-set RSS.
Make $T_{\alpha}$ the final tree.

Pruning Regression Trees
========================

```{r, echo=FALSE}
set.seed(1234)
train_indices <- sample(nrow(Auto), nrow(Auto)/2)
train_set <- Auto[train_indices,]
test_set <- Auto[-train_indices,]

auto_tree <- tree(mpg~cylinders+displacement+horsepower+weight+acceleration+year+factor(origin), data=train_set)
plot(auto_tree)
text(auto_tree, pretty=0, cex=1.4)
```

***
  
```{r, echo=FALSE}
cv_auto <- cv.tree(auto_tree)
plot(cv_auto$size, cv_auto$dev, type="b", xlab="Tree Size", ylab="RSS")
```

Pruning Regression Trees
=========================

```{r, echo=FALSE}
plot(train_set$mpg, predict(auto_tree, newdata=train_set), xlab="Observed MPG", ylab="Predicted MPG", main="Full Tree Training Error")
abline(0,1)

rmse <- sqrt( mean( (train_set$mpg - predict(auto_tree, newdata=train_set))^2 ))
legend("bottomright", legend=paste("RMSE=", round(rmse, digits=2)), cex=2)
```

***
```{r, echo=FALSE}
pruned_auto <- prune.tree(auto_tree, best=5)
plot(train_set$mpg, predict(pruned_auto, newdata=train_set), xlab="Observed MPG", ylab="Predicted MPG", main="Pruned Tree Training Error")
abline(0,1)

rmse <- sqrt( mean( (train_set$mpg - predict(pruned_auto, newdata=train_set))^2 ))
legend("bottomright", legend=paste("RMSE=", round(rmse, digits=2)), cex=2)
```

Pruning Regression Trees
=========================

```{r, echo=FALSE}
plot(test_set$mpg, predict(auto_tree, newdata=test_set), xlab="Observed MPG", ylab="Predicted MPG", main="Full Tree Testing Error")
abline(0,1)

rmse <- sqrt( mean( (test_set$mpg - predict(auto_tree, newdata=test_set))^2 ))
legend("bottomright", legend=paste("RMSE=", round(rmse, digits=2)), cex=2)
```

***

```{r, echo=FALSE}
plot(test_set$mpg, predict(pruned_auto, newdata=test_set), xlab="Observed MPG", ylab="Predicted MPG", main="Pruned Tree Testing Error")
abline(0,1)

rmse <- sqrt( mean( (test_set$mpg - predict(pruned_auto, newdata=test_set))^2 ))
legend("bottomright", legend=paste("RMSE=", round(rmse, digits=2)), cex=2)
```

Classification Trees
=====================

- Same partitioning principle, but now, each region predicts the majority class for training observations within region.
- Naive approach: looking for partitions that minimize training error
- Better performing approaches: look for partitions that minimize (for leaf $m$):
  - **Gini Index**: $\sum_{k=1}^K \hat{p}_{mk}(1-\hat{p}_{mk})$, or
  - **Entropy**: $-\sum_{k=1}^K \hat{p}_{mk}\log(\hat{p}_{mk})$
  
where $\hat{p}_{mk}$ is the proportion of training observations in partition $m$ labeled as class $k$.
- Both of these seek to partition observations into subsets that have the same labels.

Classification Trees
=====================

```{r, echo=FALSE}
data(Default)

with(Default, {
     plot(balance, income, pch=ifelse(student=="Yes", 19, 21), col=default)
     legend("topright", pch=c(19,21,19,19), col=c("black","black",1,2), legend=c("Student", "Not Student","Not Default","Default"))
})
```

***

```{r, echo=FALSE}
default_tree <- tree(default~student+balance+income, data=Default)
plot(default_tree)
text(default_tree, pretty=0)
```

Classification Trees
=====================

```{r, echo=FALSE}
default_tree
```

Decision Trees
===============

Advantages

- Highly interpretable, even moreso than linear models  
- Easy to visualize (if small enough)  
- Maybe models human decision processes?  
- No dummy predictors for categorical variables

Decision Trees
===============

Disadvantages

- Greedy approach via recursive partitioning
- Not always best performing (not very flexible)
- Highly unstable to changes in training data

Random Forests
===============

A **very popular** approach that addresses these shortcomings via resampling:

Goal is to improve prediction performance and reduce instability by _averaging_ multiple decision trees (a forest constructed with randomness)

Random Forests
===============

First trick: *Bagging* (bootstrap aggregation)
General scheme:
  1. Build many decision trees $T_1, T_2, \ldots, T_B$ from training set
  2. Given a new observation, let each $T_j$ predict $\hat{y}_j$
  3. For regression: predict average $\frac{1}{B} \sum_{j=1}^B \hat{y}_j$,
     for classification: predict with majority vote (most frequent class)
     
But wait, how do we get many decision trees from a single training set?

Bootstrap
=========

Details in Ch. 5, but a very general resampling technique

To create $T_j, \, j=1,\ldots,B$ from training set of size $n$:

a) create a bootstrap training set by sampling $n$ observations from training set **with replacement**
b) build a decision tree from bootstrap training set

Bootstrap
==========

![](bootstrap.png)

Random Forests
===============

Second trick: random selection of features to split!

When building each tree $T_j$, at each recursive partition only consider a randomly selected subset of predictors to check for best split

  - This reduces correlation between trees in forest, improving prediction accuracy

Random Forests
================

```{r, echo=FALSE}
set.seed(1234)
train_indices <- sample(nrow(Auto), nrow(Auto)/2)
train_set <- Auto[train_indices,]
test_set <- Auto[-train_indices,]

library(randomForest)

auto_rf <- randomForest(mpg~cylinders+displacement+horsepower+weight+acceleration+year+origin, importance=TRUE, mtry=3, data=train_set)

plot(train_set$mpg, predict(auto_rf, newdata=train_set), xlab="Observed MPG", ylab="Predicted MPG", main="RF Training Error")
abline(0,1)

rmse <- sqrt( mean( (train_set$mpg - predict(auto_rf, newdata=train_set) )^2 ))

legend("bottomright", legend=paste("RMSE=", round(rmse, digits=2)), cex=2)
```

***

```{r, echo=FALSE}
plot(test_set$mpg, predict(auto_rf, newdata=test_set), xlab="Observed MPG", ylab="Predicted MPG", main="RF Testing Error")
abline(0,1)

rmse <- sqrt( mean( (test_set$mpg - predict(auto_rf, newdata=test_set) )^2 ))

legend("bottomright", legend=paste("RMSE=", round(rmse, digits=2)), cex=2)
```

Random Forests
===============

A disadvantage is that we lose interpretability...

But there are methods to measure _variable importance_ from the random forest.

```{r, echo=FALSE, results="asis"}
variable_importance <- importance(auto_rf)
kable(head(round(variable_importance, digits=2)))
```

***

```{r, echo=FALSE}
imp <- importance(auto_rf)[,2]
par(mar=par()$mar+c(0,5,0,0))
o <- order(imp)
barplot(imp[o], horiz=TRUE, xlab="Variable Importance", las=2, cex.names=1.6)
```

Tree-Based Methods
===================

Summary

- Interpretable _prediction_ models
- Some inferential tasks possible (variable importance)
- Very commonly used
- Random Forests perform at state-of-the-art for many tasks
